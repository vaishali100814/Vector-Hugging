{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Assignment"
      ],
      "metadata": {
        "id": "2lIcPl46hcpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Computational Linguistics and how does it relate to NLP?\n",
        "  - Computational linguistics is an interdisciplinary field that focuses on the computational aspects of language, while natural language processing (NLP) is a subfield that applies these principles to build practical applications. Computational linguistics is more theoretical, exploring the underlying linguistic structures and rules that computers must understand, whereas NLP is applied, aiming to create systems like chatbots and machine translators that can process and generate human language. The relationship is symbiotic; computational linguistics provides the theoretical foundation that enables NLP applications, and NLP's practical work provides data and insights that inform computational linguistics research.\n",
        "\n",
        "Question 2: Briefly describe the historical evolution of Natural Language Processing.\n",
        "  - The historical evolution of Natural Language Processing (NLP) progressed through three major paradigms: rule-based systems, statistical methods, and the modern deep learning revolution. This journey reflects a shift from human-coded instructions to machine learning models that learn from vast amounts of data.\n",
        "  The Dawn of NLP: Rule-Based Systems (1950s-1970s):\n",
        "  Key Milestones: The 1954 Georgetown-IBM experiment successfully translated 60 Russian sentences into English using a system that matched words and applied basic rules. In the mid-1960s, the chatbot ELIZA simulated conversation by recognizing keywords and using simple pattern-matching responses, though it did not genuinely \"understand\" human language.\n",
        "  Limitations: These systems struggled with the inherent ambiguity, nuances, and exceptions of human language, leading to an \"AI winter\" in NLP research after the 1966 ALPAC report deemed machine translation efforts ineffective.\n",
        "  The Statistical Revolution (1980s-2000s):\n",
        "  Key Milestones: Techniques like Hidden Markov Models (HMMs) became crucial for speech recognition and part-of-speech tagging. In the 2000s, the development of neural language models and word embeddings (e.g., Word2Vec) allowed machines to represent words as numerical vectors that captured semantic relationships, fundamentally changing how language was processed.\n",
        "  Applications: This era saw the rise of more practical applications, including improved search engines, spam filters, and the launch of Google Translate in 2006, which used statistical machine translation.  \n",
        "\n",
        "Question 3: List and explain three major use cases of NLP in today’s tech industry.\n",
        "  - Text Classification:\n",
        "  One major use case of Natural Language Processing (NLP) is text classification, which involves programmatically categorizing textual content into predefined groups. This is used extensively in industry for things like spam detection in email and sentiment analysis on social media reviews, allowing systems to automatically filter content or gauge public opinion at scale.\n",
        "  Named Entity Recognition (NER):\n",
        "  Another key application is Named Entity Recognition (NER), a process that identifies and categorizes key information (entities) within unstructured text. This powers applications such as customer support systems that automatically extract crucial details like product names, tracking numbers, or personal details, streamlining data entry and improving search functionality.\n",
        "  Machine Translation:\n",
        "  Finally, machine translation utilizes NLP to automatically translate text or speech from one natural language to another while aiming to preserve both meaning and context. Services like Google Translate rely on sophisticated NLP models to break down source language syntax and semantics, enabling seamless global communication for individuals and businesses alike.\n",
        "\n",
        "Question 4: What is text normalization and why is it essential in text processing tasks?\n",
        "  - Text normalization is a pre-processing step that converts raw text into a consistent, standard format to improve processing efficiency and accuracy. It is essential because it reduces complexity by handling variations like different cases (e.g., \"Hello\" and \"hello\"), punctuation, and word forms (e.g., \"running\" and \"ran\"), allowing models to treat them as the same. This standardization reduces vocabulary size, decreases dimensionality, and improves the model's ability to generalize and perform better across various Natural Language Processing (NLP) tasks.\n",
        "  Text normalization involves a series of techniques to standardize text into a consistent format, such as:\n",
        "  Case normalization: Converting all letters to a single case, usually lowercase, so that \"Apple\" and \"apple\" are treated as the same word.\n",
        "  Punctuation removal: Removing punctuation marks like commas, periods, and question marks, which may not be relevant for many analyses.\n",
        "  Punctuation removal: Removing punctuation marks like commas, periods, and question marks, which may not be relevant for many analyses.\n",
        "  Stemming and lemmatization: Reducing words to their root or base form. Stemming chops off endings (e.g., \"running\" becomes \"run\"), while lemmatization uses a dictionary to find the base form (lemma).\n",
        "  Stop word removal: Removing common words like \"a,\" \"the,\" and \"is,\" which often do not carry significant meaning.\n",
        "  It is essential because:\n",
        "  Improves accuracy: By standardizing words like \"run,\" \"runs,\" and \"running\" into a single form, models can better understand the underlying meaning, leading to more accurate results.\n",
        "  Reduces data complexity: Normalization significantly reduces the number of unique words (vocabulary), which is crucial for models that have limitations on vocabulary size.\n",
        "  Decreases dimensionality: With fewer unique terms, the complexity of the data is reduced, making it more efficient for models to process and analyze.\n",
        "  Enhances generalization: By treating variations of the same word identically, models can generalize better and are less likely to be confused by different but semantically identical inputs.\n",
        "  Increases efficiency: Reducing the size and complexity of the data allows machine learning models to train faster and more efficiently.\n",
        "\n",
        "Question 5: Compare and contrast stemming and lemmatization with suitable\n",
        "examples.\n",
        "  - Stemming is a faster, rule-based process that chops off word endings to get a root form, which may not be a valid dictionary word. Lemmatization uses a dictionary and morphological analysis to return the valid, base or dictionary form (lemma) of a word, considering its context.\n",
        "  Comparison of Stemming and Lemmatization\n",
        "  Both techniques aim to reduce inflectional forms of words to a common base form to aid in text processing tasks like information retrieval and text classification. However, their methodologies and outputs differ significantly.\n",
        "  Aspect \tLemmatization\tStemming\n",
        "  Methodology\tUses a dictionary and morphological analysis.\tUses heuristic, rule-based algorithms (like Porter or Snowball stemmers) to strip affixes.\n",
        "  Output\tThe output, called a lemma, is always a meaningful word found in a dictionary.\tThe output, called a stem, may not be a linguistically valid or actual word.\n",
        "  Context\tIt is context-aware, often requiring the word's part of speech (POS) to determine the correct base form.\tIt is context-agnostic, operating on a single word by applying rules without considering the surrounding text or meaning.\n",
        "  Speed\tSlower and more computationally intensive due to dictionary lookups and complex analysis.\tFaster and less computationally intensive due to simple rule application.\n",
        "  Accuracy\tGenerally more accurate and suitable for tasks requiring deep language understanding.\tLess accurate and prone to errors like over-stemming (unrelated words reduced to the same stem) or under-stemming (related words not reduced).\n",
        "          \n"
      ],
      "metadata": {
        "id": "T0FrzilihQVo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJoByMYlhMQt",
        "outputId": "52cb1648-e385-445b-bde8-3d9d59338f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid email\n",
            "Invalid email\n",
            "Invalid email\n",
            "Invalid email\n"
          ]
        }
      ],
      "source": [
        "# Question 6: Write a Python program that uses regular expressions (regex) to extract all email addresses from the following block of text:\n",
        "\n",
        "import re\n",
        "\n",
        "# Compiling the regex pattern for email validation\n",
        "regex = re.compile(\n",
        "    r\"(?i)\"  # Case-insensitive matching\n",
        "    r\"(?:[A-Z0-9!#$%&'*+/=?^_`{|}~-]+\"  # Unquoted local part\n",
        "    r\"(?:\\.[A-Z0-9!#$%&'*+/=?^_`{|}~-]+)*\"  # Dot-separated atoms in local part\n",
        "    r\"|\\\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]\"  # Quoted strings\n",
        "    r\"|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\\\")\"  # Escaped characters in local part\n",
        "    r\"@\"  # Separator\n",
        "    r\"[A-Z0-9](?:[A-Z0-9-]*[A-Z0-9])?\"  # Domain name\n",
        "    r\"\\.(?:[A-Z0-9](?:[A-Z0-9-]*[A-Z0-9])?)+\"  # Top-level domain and subdomains\n",
        ")\n",
        "\n",
        "def isValid(email):\n",
        "    \"\"\"Check if the given email address is valid.\"\"\"\n",
        "    return \"Valid email\" if re.fullmatch(regex, email) else \"Invalid email\"\n",
        "\n",
        "# Example Usage\n",
        "print(isValid(\"name.surname@gmail.com\"))\n",
        "print(isValid(\"anonymous123@yahoo.co.uk\"))\n",
        "print(isValid(\"anonymous123@...uk\"))\n",
        "print(isValid(\"...@domain.us\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Given the sample paragraph below, perform string tokenization and frequency distribution using Python and NLTK:\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Ensure you have the 'punkt' tokenizer models downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Sample paragraph\n",
        "paragraph = \"Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\"\n",
        "\n",
        "# 1. Tokenization\n",
        "tokens = word_tokenize(paragraph)\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "# 2. Frequency Distribution\n",
        "fdist = FreqDist(tokens)\n",
        "print(\"\\nFrequency Distribution (Top 10):\")\n",
        "print(fdist.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "Q5QSQ-Ton_Al",
        "outputId": "331b2848-dab0-4832-8332-c7d226e1e287"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'nltk.downloader' has no attribute 'DownloadError'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1989515071.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1989515071.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mexcept\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'nltk.downloader' has no attribute 'DownloadError'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Create a custom annotator using spaCy or NLTK that identifies and labels proper nouns in a given text.\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_proper_nouns_spacy(text):\n",
        "    \"\"\"\n",
        "    Identifies and labels proper nouns in a given text using spaCy.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    proper_nouns = []\n",
        "    for token in doc:\n",
        "        # Proper nouns are typically tagged as 'NNP' (singular) or 'NNPS' (plural)\n",
        "        # or identified as part of a Named Entity (e.g., PERSON, GPE, ORG)\n",
        "        if token.pos_ == \"PROPN\" or token.ent_type_ != \"\":\n",
        "            proper_nouns.append((token.text, \"PROPER_NOUN\"))\n",
        "    return proper_nouns\n",
        "\n",
        "# Example usage\n",
        "text = \"Apple Inc. is a technology company based in Cupertino, California. Tim Cook is the CEO.\"\n",
        "identified_nouns = extract_proper_nouns_spacy(text)\n",
        "print(identified_nouns)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVJTnuBTBSec",
        "outputId": "88a718fe-1b3a-4944-9164-48d734117acd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Apple', 'PROPER_NOUN'), ('Inc.', 'PROPER_NOUN'), ('Cupertino', 'PROPER_NOUN'), ('California', 'PROPER_NOUN'), ('Tim', 'PROPER_NOUN'), ('Cook', 'PROPER_NOUN'), ('CEO', 'PROPER_NOUN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Using Genism, demonstrate how to train a simple Word2Vec model on the following dataset consisting of example sentences.\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "import nltk\n",
        "# Ensure you have the necessary NLTK data (optional, for advanced cleaning)\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# 1. Define the dataset\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# 2. Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove non-alphanumeric characters and extra spaces\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenize (split into words) and remove short words\n",
        "    tokens = [word for word in text.split() if len(word) > 2]\n",
        "    return tokens\n",
        "\n",
        "# 3. Tokenize and preprocess the entire dataset\n",
        "processed_sentences = [preprocess_text(sentence) for sentence in dataset]\n",
        "\n",
        "# Display the processed sentences\n",
        "print(\"Processed sentences (tokens):\")\n",
        "for sentence in processed_sentences:\n",
        "    print(sentence)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 4. Train the Word2Vec model\n",
        "# Parameters explained:\n",
        "#   sentences: the pre-processed list of sentences (list of lists of words)\n",
        "#   vector_size: Dimensionality of the word vectors (e.g., 20)\n",
        "#   window: Maximum distance between the current and predicted word within a sentence (e.g., 5)\n",
        "#   min_count: Ignores all words with a frequency lower than this (e.g., 1)\n",
        "#   sg: Training algorithm: 0 for CBOW (default), 1 for Skip-gram (e.g., 1 here)\n",
        "model = Word2Vec(sentences=processed_sentences, vector_size=20, window=5, min_count=1, sg=1)\n",
        "\n",
        "# 5. Build the vocabulary\n",
        "model.build_vocab(processed_sentences)\n",
        "\n",
        "# 6. Train the model (optional if already passed in constructor, but good practice for clarity)\n",
        "# total_examples=model.corpus_count, epochs=model.epochs\n",
        "model.train(processed_sentences, total_examples=len(processed_sentences), epochs=10)\n",
        "\n",
        "# 7. Demonstrate the model (optional)\n",
        "print(\"Model vocabulary size:\", len(model.wv.index_to_key))\n",
        "print(\"Vector for the word 'language':\\n\", model.wv['language'])\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Example of finding similar words\n",
        "try:\n",
        "    similar_words = model.wv.most_similar('word2vec', topn=2)\n",
        "    print(\"Words most similar to 'word2vec':\", similar_words)\n",
        "except KeyError as e:\n",
        "    print(f\"Word not in vocabulary or not enough context to find similarities: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "bmV2kMr_FVBr",
        "outputId": "ccc32547-2531-4576-df31-4f5712fffb3a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2411318282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Question 9: Using Genism, demonstrate how to train a simple Word2Vec model on the following dataset consisting of example sentences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you are a data scientist at a fintech startup. You’ve been tasked\n",
        "with analyzing customer feedback. Outline the steps you would take to clean, process,\n",
        "and extract useful insights using NLP techniques from thousands of customer reviews.\n",
        "  - Data Cleaning and Preprocessing\n",
        "To begin, you must clean and preprocess the raw text data to make it suitable for NLP analysis. [1] This involves several crucial steps to standardize the text.\n",
        "Remove Noise: Eliminate irrelevant information such as HTML tags, URLs, numbers, and special characters, as these do not contribute to the semantic meaning of the reviews.\n",
        "Case Conversion: Convert all text to lowercase to ensure consistency and prevent the same word in different cases from being treated as separate tokens.\n",
        "Tokenization: Break down the continuous text into individual words or phrases (tokens) which are the fundamental units for NLP.\n",
        "Stop Word Removal: Filter out common, non-informative words like \"the,\" \"a,\" \"is,\" which are frequent but carry little analytical value. [1]\n",
        "Lemmatization/Stemming: Reduce words to their base or root form (e.g., \"running,\" \"runs,\" and \"ran\" all become \"run\"). Lemmatization is often preferred as it uses a dictionary to return a valid word (lemma), ensuring better semantic integrity than simple stemming. [1]\n",
        "Feature Engineering and Insight Extraction\n",
        "Once the data is clean, you can use various NLP techniques to extract meaningful insights. These methods transform the text into numerical formats that machine learning models can understand.\n",
        "Sentiment Analysis: Use models to classify reviews as positive, negative, or neutral. [1] This provides an overall understanding of customer satisfaction and helps identify general trends in feedback.\n",
        "Topic Modeling: Employ techniques like Latent Dirichlet Allocation (LDA) to automatically discover abstract \"topics\" present in the review collection. [1] This helps identify common themes, such as \"app usability,\" \"fees,\" or \"customer support,\" allowing you to pinpoint specific areas for improvement.\n",
        "Text Vectorization: Convert the processed text into numerical vectors using methods like TF-IDF (Term Frequency-Inverse Document Frequency) or modern embeddings (word embeddings). [1] This numerical representation is essential for quantitative analysis and subsequent modeling.\n"
      ],
      "metadata": {
        "id": "WRoCLo2vHTcL"
      }
    }
  ]
}